{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJn9QRSPJIhf"
      },
      "source": [
        "# Introduction to Data Science 2025\n",
        "\n",
        "# Week 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mdF2UcuJIhh"
      },
      "source": [
        "In this week's exercise, we look at prompting and zero- and few-shot task settings. Below is a text generation example from https://github.com/TurkuNLP/intro-to-nlp/blob/master/text_generation_pipeline_example.ipynb demonstrating how to load a text generation pipeline with a pre-trained model and generate text with a given prompt. Your task is to load a similar pre-trained generative model and assess whether the model succeeds at a set of tasks in zero-shot, one-shot, and two-shot settings.\n",
        "\n",
        "**Note: Downloading and running the pre-trained model locally may take some time. Alternatively, you can open and run this notebook on [Google Colab](https://colab.research.google.com/), as assumed in the following example.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIQ1s96UCcJW"
      },
      "source": [
        "## Text generation example\n",
        "\n",
        "This is a brief example of how to run text generation with a causal language model and `pipeline`.\n",
        "\n",
        "Install [transformers](https://huggingface.co/docs/transformers/index) python package. This will be used to load the model and tokenizer and to run generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4fUBJmXHCHw-"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZRNZgRJCt6Q"
      },
      "source": [
        "Import the `AutoTokenizer`, `AutoModelForCausalLM`, and `pipeline` classes. The first two support loading tokenizers and generative models from the [Hugging Face repository](https://huggingface.co/models), and the last wraps a tokenizer and a model for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jwyK005xCFSF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QJPDe3ZC_sL"
      },
      "source": [
        "Load a generative model and its tokenizer. You can substitute any other generative model name here (e.g. [other TurkuNLP GPT-3 models](https://huggingface.co/models?sort=downloads&search=turkunlp%2Fgpt3)), but note that Colab may have issues running larger models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqTxn_QaCNjZ"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'TurkuNLP/gpt3-finnish-large'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ADWWb77e1sY"
      },
      "source": [
        "Instantiate a text generation pipeline using the tokenizer and model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IIJzNrEe5qx"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\n",
        "    'text-generation',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=model.device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAohNr1ciwaU"
      },
      "source": [
        "We can now call the pipeline with a text prompt; it will take care of tokenizing, encoding, generation, and decoding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWcOJkiKi5vr"
      },
      "outputs": [],
      "source": [
        "output = pipe('Terve, miten menee?', max_new_tokens=25)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNRMsxXOjSo0"
      },
      "source": [
        "Just print the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Op7MJ6XjahG"
      },
      "outputs": [],
      "source": [
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YROp3hyikXPO"
      },
      "source": [
        "We can also call the pipeline with any arguments that the model `generate` function supports. For details on text generation using `transformers`, see e.g. [this tutorial](https://huggingface.co/blog/how-to-generate).\n",
        "\n",
        "Example with sampling and a high `temperature` parameter to generate more chaotic output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22QjXE88jkim"
      },
      "outputs": [],
      "source": [
        "output = pipe(\n",
        "    'Terve, miten menee?',\n",
        "    do_sample=True,\n",
        "    temperature=10.0,\n",
        "    max_new_tokens=25\n",
        ")\n",
        "\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SB4TesKJIhk"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "Your task is to assess whether a generative model succeeds in the following tasks in zero-shot, one-shot, and two-shot settings:\n",
        "\n",
        "- binary sentiment classification (positive / negative)\n",
        "\n",
        "- person name recognition\n",
        "\n",
        "- two-digit addition (e.g. 11 + 22 = 33)\n",
        "\n",
        "For example, for assessing whether a generative model can name capital cities, we could use the following prompts:\n",
        "\n",
        "- zero-shot:\n",
        "\t>\"\"\"\\\n",
        "\t>Identify the capital cities of countries.\n",
        "\t>\n",
        "\t>Question: What is the capital of Finland?\\\n",
        "\t>Answer:\\\n",
        "\t>\"\"\"\n",
        "- one-shot:\n",
        "\t>\"\"\"\\\n",
        "\t>Identify the capital cities of countries.\n",
        "\t>\n",
        "\t>Question: What is the capital of Sweden?\\\n",
        "\t>Answer: Stockholm\n",
        "\t>\n",
        "\t>Question: What is the capital of Finland?\\\n",
        "\t>Answer:\\\n",
        "\t>\"\"\"\n",
        "- two-shot:\n",
        "\t>\"\"\"\\\n",
        "\t>Identify the capital cities of countries.\n",
        "\t>\n",
        "\t>Question: What is the capital of Sweden?\\\n",
        "\t>Answer: Stockholm\n",
        "\t>\n",
        "\t>Question: What is the capital of Denmark?\\\n",
        "\t>Answer: Copenhagen\n",
        "\t>\n",
        "\t>Question: What is the capital of Finland?\\\n",
        "\t>Answer:\\\n",
        "\t>\"\"\"\n",
        "\n",
        "You can do the tasks either in English or Finnish and use a generative model of your choice from the Hugging Face models repository, for example the following models:\n",
        "\n",
        "- English: `gpt2-large`\n",
        "- Finnish: `TurkuNLP/gpt3-finnish-large`\n",
        "\n",
        "You can either come up with your own instructions for the tasks or use the following:\n",
        "\n",
        "- English:\n",
        "\t- binary sentiment classification: \"Do the following texts express a positive or negative sentiment?\"\n",
        "\t- person name recognition: \"List the person names occurring in the following texts.\"\n",
        "\t- two-digit addition: \"This is a first grade math exam.\"\n",
        "- Finnish:\n",
        "\t- binary sentiment classification: \"Ilmaisevatko seuraavat tekstit positiivista vai negatiivista tunnetta?\"\n",
        "\t- person name recognition: \"Listaa seuraavissa teksteissä mainitut henkilönnimet.\"\n",
        "\t- two-digit addition: \"Tämä on ensimmäisen luokan matematiikan koe.\"\n",
        "\n",
        "Come up with at least two test cases for each of the three tasks, and come up with your own one- and two-shot examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0S_Nj_5JIhk",
        "outputId": "24a7aef5-c26d-4623-9412-060e5f5a6db1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary sentiment classification\n",
            "\n",
            "Zero-shot:\n",
            "  I love this book, it's amazing!: I\n",
            "  This is terrible and disappointing.: This\n",
            "\n",
            "One-shot:\n",
            "  I love this book, it's amazing!: positive\n",
            "  This is terrible and disappointing.: negative\n",
            "\n",
            "Two-shot:\n",
            "  I love this book, it's amazing!: positive\n",
            "  This is terrible and disappointing.: negative\n",
            "\n",
            "\n",
            "Person name recognition\n",
            "\n",
            "Zero-shot:\n",
            "  Alice and Bob went to cinema.: Alice and Bob went to cinema.\n",
            "  Cindy met Tom yesterday.: Cindy, Tom, Tom, Tom, Tom,\n",
            "\n",
            "One-shot:\n",
            "  Alice and Bob went to cinema.: Alice, Bob\n",
            "  Cindy met Tom yesterday.: Cindy, Tom\n",
            "\n",
            "Two-shot:\n",
            "  Alice and Bob went to cinema.: Alice, Bob\n",
            "  Cindy met Tom yesterday.: Cindy, Tom\n",
            "\n",
            "\n",
            "Two-digit addition\n",
            "\n",
            "Zero-shot:\n",
            "  21 + 55 = 21\n",
            "  17 + 19 = 17\n",
            "\n",
            "One-shot:\n",
            "  21 + 55 = 89\n",
            "  17 + 19 = 33\n",
            "\n",
            "Two-shot:\n",
            "  21 + 55 = 75\n",
            "  17 + 19 = 36\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "MODEL_NAME = 'gpt2-large'\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device='cpu')\n",
        "\n",
        "print(\"Binary sentiment classification\\n\")\n",
        "\n",
        "sentiment_tests = [\n",
        "    \"I love this book, it's amazing!\",\n",
        "    \"This is terrible and disappointing.\"\n",
        "]\n",
        "\n",
        "print(\"Zero-shot:\")\n",
        "for text in sentiment_tests:\n",
        "    prompt = f\"Do the following texts express a positive or negative sentiment?\\n\\nText: {text}\\nSentiment:\"\n",
        "    result = pipe(prompt, max_new_tokens=3, temperature=0.1)[0]['generated_text']\n",
        "    answer = result[len(prompt):].strip().split()[0]\n",
        "    print(f\"  {text}: {answer}\")\n",
        "\n",
        "print(\"\\nOne-shot:\")\n",
        "for text in sentiment_tests:\n",
        "    prompt = f\"\"\"Do the following texts express a positive or negative sentiment?\n",
        "\n",
        "Text: This is wonderful!\n",
        "Sentiment: positive\n",
        "\n",
        "Text: {text}\n",
        "Sentiment:\"\"\"\n",
        "    result = pipe(prompt, max_new_tokens=3, temperature=0.1)[0]['generated_text']\n",
        "    answer = result[len(prompt):].strip().split()[0]\n",
        "    print(f\"  {text}: {answer}\")\n",
        "\n",
        "print(\"\\nTwo-shot:\")\n",
        "for text in sentiment_tests:\n",
        "    prompt = f\"\"\"Do the following texts express a positive or negative sentiment?\n",
        "\n",
        "Text: This is wonderful!\n",
        "Sentiment: positive\n",
        "\n",
        "Text: I hate this.\n",
        "Sentiment: negative\n",
        "\n",
        "Text: {text}\n",
        "Sentiment:\"\"\"\n",
        "    result = pipe(prompt, max_new_tokens=3, temperature=0.1)[0]['generated_text']\n",
        "    answer = result[len(prompt):].strip().split()[0]\n",
        "    print(f\"  {text}: {answer}\")\n",
        "\n",
        "print(\"\\n\\nPerson name recognition\\n\")\n",
        "\n",
        "name_tests = [\n",
        "    \"Alice and Bob went to cinema.\",\n",
        "    \"Cindy met Tom yesterday.\"\n",
        "]\n",
        "\n",
        "print(\"Zero-shot:\")\n",
        "for text in name_tests:\n",
        "    prompt = f\"List the person names occurring in the following texts.\\n\\nText: {text}\\nNames:\"\n",
        "    result = pipe(prompt, max_new_tokens=10, temperature=0.1)[0]['generated_text']\n",
        "    answer = result[len(prompt):].strip().split('\\n')[0]\n",
        "    print(f\"  {text}: {answer}\")\n",
        "\n",
        "print(\"\\nOne-shot:\")\n",
        "for text in name_tests:\n",
        "    prompt = f\"\"\"List the person names occurring in the following texts.\n",
        "\n",
        "Text: John and Mary are friends.\n",
        "Names: John, Mary\n",
        "\n",
        "Text: {text}\n",
        "Names:\"\"\"\n",
        "    result = pipe(prompt, max_new_tokens=10, temperature=0.1)[0]['generated_text']\n",
        "    answer = result[len(prompt):].strip().split('\\n')[0]\n",
        "    print(f\"  {text}: {answer}\")\n",
        "\n",
        "print(\"\\nTwo-shot:\")\n",
        "for text in name_tests:\n",
        "    prompt = f\"\"\"List the person names occurring in the following texts.\n",
        "\n",
        "Text: John and Mary are friends.\n",
        "Names: John, Mary\n",
        "\n",
        "Text: Sarah called David yesterday.\n",
        "Names: Sarah, David\n",
        "\n",
        "Text: {text}\n",
        "Names:\"\"\"\n",
        "    result = pipe(prompt, max_new_tokens=10, temperature=0.1)[0]['generated_text']\n",
        "    answer = result[len(prompt):].strip().split('\\n')[0]\n",
        "    print(f\"  {text}: {answer}\")\n",
        "\n",
        "print(\"\\n\\nTwo-digit addition\\n\")\n",
        "\n",
        "math_tests = [\n",
        "    \"21 + 55\",\n",
        "    \"17 + 19\"\n",
        "]\n",
        "\n",
        "print(\"Zero-shot:\")\n",
        "for problem in math_tests:\n",
        "    prompt = f\"This is a first grade math exam.\\n\\nProblem: {problem} =\\nAnswer:\"\n",
        "    result = pipe(prompt, max_new_tokens=5, temperature=0.1)[0]['generated_text']\n",
        "    answer = result[len(prompt):].strip().split()[0]\n",
        "    print(f\"  {problem} = {answer}\")\n",
        "\n",
        "print(\"\\nOne-shot:\")\n",
        "for problem in math_tests:\n",
        "    prompt = f\"\"\"This is a first grade math exam.\n",
        "\n",
        "Problem: 11 + 33 = 44\n",
        "\n",
        "Problem: {problem} =\"\"\"\n",
        "    result = pipe(prompt, max_new_tokens=5, temperature=0.1)[0]['generated_text']\n",
        "    answer = result[len(prompt):].strip().split()[0]\n",
        "    print(f\"  {problem} = {answer}\")\n",
        "\n",
        "print(\"\\nTwo-shot:\")\n",
        "for problem in math_tests:\n",
        "    prompt = f\"\"\"This is a first grade math exam.\n",
        "\n",
        "Problem: 11 + 33 = 44\n",
        "Problem: 20 + 18 = 38\n",
        "\n",
        "Problem: {problem} =\"\"\"\n",
        "    result = pipe(prompt, max_new_tokens=5, temperature=0.1)[0]['generated_text']\n",
        "    answer = result[len(prompt):].strip().split()[0]\n",
        "    print(f\"  {problem} = {answer}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.13.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}